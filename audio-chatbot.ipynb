{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb270671-2f8d-4460-93e2-0db2547f7e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from torchaudio.sox_effects import apply_effects_file\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioXVector\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "import gradio\n",
    "from speechbrain.pretrained import EncoderDecoderASR\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105d3674-2754-47a5-b7ee-33fa88c0d5e1",
   "metadata": {},
   "source": [
    "![Architecture](AudioArchitecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08386063-3b71-4220-9eed-85f43c731664",
   "metadata": {},
   "source": [
    "![Architecture](Speechbrain.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "049eaa06-7147-47e8-89e7-dc0d42d65377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.pretrained import Tacotron2\n",
    "from speechbrain.pretrained import HIFIGAN\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "537e4a00-1fb7-48cb-bd1c-70b5d37b5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854b2d82-8dfa-4b66-993e-13be0f2f0dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "transformer = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "question_answerer = pipeline(\"question-answering\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add8b01c-37cf-44ce-b111-f05173b96420",
   "metadata": {},
   "outputs": [],
   "source": [
    "EFFECTS = [\n",
    "    [\"remix\", \"-\"],\n",
    "    [\"channels\", \"1\"],\n",
    "    [\"rate\", \"16000\"],\n",
    "    [\"gain\", \"-1.0\"],\n",
    "    [\"silence\", \"1\", \"0.1\", \"0.1%\", \"-1\", \"0.1\", \"0.1%\"],\n",
    "    [\"trim\", \"0\", \"10\"],\n",
    "]\n",
    "\n",
    "THRESHOLD = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0511d409-cc82-4159-ba30-a072f9f7bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models are located in /home/jupyter/.cache/huggingface/hub\n",
    "model_name = \"microsoft/unispeech-sat-base-plus-sv\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "model = AutoModelForAudioXVector.from_pretrained(model_name).to(device)\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddb90a91-b96f-44da-a1c6-6697d5a7c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_generator = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7362a14-35b2-410b-a503-2cde7bab56bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 is frozen.\n"
     ]
    }
   ],
   "source": [
    "from speechbrain.pretrained.interfaces import foreign_class\n",
    "classifier = foreign_class(source=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", pymodule_file=\"custom_interface.py\", classname=\"CustomEncoderWav2vec2Classifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95999eda-fae6-4d7b-a71d-68d97817f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize TTS (tacotron2) and Vocoder   warnings.warn(\n",
    "#(HiFIGAN)\n",
    "tacotron2 = Tacotron2.from_hparams(source=\"speechbrain/tts-tacotron2-ljspeech\", savedir=\"tmpdir_tts\")\n",
    "hifi_gan = HIFIGAN.from_hparams(source=\"speechbrain/tts-hifigan-ljspeech\", savedir=\"tmpdir_vocoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "debdd445-f9b2-4aeb-8962-7bb97f7ae387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 is frozen.\n"
     ]
    }
   ],
   "source": [
    "asr_model = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-wav2vec2-commonvoice-en\", savedir=\"pretrained_models/asr-wav2vec2-commonvoice-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd81f3a-efde-4b85-9aaf-d7ee7359c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_generator = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f02831cb-5a0e-4fbe-9e5d-56eac4d8ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(path):\n",
    "    return asr_model.transcribe_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1b78829-888a-42b5-8f88-a1e3573939fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(audio):\n",
    "    text=getText(audio)\n",
    "    return text,transformer.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f949f0db-19eb-4c58-ac8d-ea92a47190a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(path1):\n",
    "    wav1, _ = apply_effects_file(path1, EFFECTS)\n",
    "    print(wav1.shape,)\n",
    "\n",
    "    input1 = feature_extractor(wav1.squeeze(0), return_tensors=\"pt\", sampling_rate=16000).input_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb1 = model(input1).embeddings\n",
    "        print(\"mb1 shape\")\n",
    "        print(emb1.shape)\n",
    "    emb1 = torch.nn.functional.normalize(emb1, dim=-1).cpu()\n",
    "    print(\"After normalize emb1 shape\")\n",
    "    print(emb1.shape)\n",
    "    return emb1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "936cfd94-b553-4d92-ab0a-ff8fa9409a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymilvus\n",
    "import librosa\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a01e44f7-94b3-461c-8836-28edce4c244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST=\"\"\n",
    "PORT=\"19530\"\n",
    "COLLECTION_NAME = 'customerdata'\n",
    "USER_COLLECTION_NAME='user'\n",
    "INDEX_TYPE = 'IVF_SQ8'\n",
    "METRIC_TYPE = 'L2'\n",
    "DIMENSION = 384\n",
    "TOPK = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d01f6357-5116-46b4-922c-1ff4a0730965",
   "metadata": {},
   "outputs": [],
   "source": [
    "connections.connect(host=HOST, port=PORT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b73243d7-8606-476a-ae45-e078202cb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "usercollection = Collection(USER_COLLECTION_NAME)\n",
    "userresult = usercollection.query(expr=\"id >= 0\",output_fields=[\"name\", \"embedding\"],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d107e10f-a634-47be-8987-80280104b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Milvus collection\n",
    "fields = [\n",
    "    FieldSchema(name='id', dtype=DataType.INT64, description='embedding ids', is_primary=True, auto_id=True),\n",
    "    FieldSchema(name='text', dtype=DataType.VARCHAR, description='user name', max_length=100),\n",
    "    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, description='text audio embeddings', dim=DIMENSION)\n",
    "    ]\n",
    "schema = CollectionSchema(fields=fields, description='Text audio embeddings')\n",
    "\n",
    "# if utility.has_collection(COLLECTION_NAME):\n",
    "#     collection = Collection(COLLECTION_NAME)\n",
    "#     collection.drop() # drop collection if it exists\n",
    "   \n",
    "collection = Collection(name=COLLECTION_NAME, schema=schema)\n",
    "\n",
    "# # Create index\n",
    "# index_params = {\n",
    "#     'metric_type': METRIC_TYPE,\n",
    "#     'index_type': INDEX_TYPE,\n",
    "#     'params':{\"nlist\":1536}\n",
    "# }\n",
    "\n",
    "# status = collection.create_index(field_name='embedding', index_params=index_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf5b06-bc14-4129-ba98-33a6aa17369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=[\"text\", \"embedding\"]\n",
    "data = []\n",
    "df=pd.DataFrame(data, columns=column_names)\n",
    "lst=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0505ec-ca8e-41aa-9a2e-e5e701142f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/jupyter/demo/addresswavinputsindiafemale/wav/\"\n",
    "for i in range(18):\n",
    "    if (i !=0 ):\n",
    "        lst.append(path+str(i)+\".wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee6350-16da-4392-833d-cd6cabd4763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7895107-0ecb-4cd8-9121-6686791e4128",
   "metadata": {},
   "outputs": [],
   "source": [
    "text, embed = get_embedding(lst[0])\n",
    "print(text)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f6e98-a981-423a-a1a4-2c4b99e4ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lst)):\n",
    "    print(lst[i])\n",
    "    text, embed=get_embedding(lst[i])\n",
    "    df.loc[len(df.index)] = [text, embed] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b516d6c3-f770-4c4b-b39e-459c0a154a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from towhee import ops, pipe, DataCollection\n",
    "\n",
    "\n",
    "insert_pipe =  (pipe.input('df')\n",
    "            .flat_map('df', 'data', lambda df: df.values.tolist())\n",
    "            .map('data', 'res', ops.ann_insert.milvus_client(host=HOST, \n",
    "                                                            port=PORT,\n",
    "                                                            collection_name=COLLECTION_NAME))\n",
    "             .output('res')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485537f7-66b0-439f-8213-d39df2c41717",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_pipe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05caf40-b0bc-417d-a996-312a24a72509",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.load()\n",
    "collection.num_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7de2ae35-e329-45a6-8d80-cc9dd476d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from towhee import ops, pipe, DataCollection\n",
    "search_pipe = (pipe.input('vec')\n",
    "                    .flat_map('vec', 'rows',  ops.ann_search.milvus_client(host=HOST, metric_type=METRIC_TYPE,\n",
    "                                                                                   port=PORT, limit= 3,reverse=True,\n",
    "                                                                                   collection_name=COLLECTION_NAME, **{'output_fields': ['text']}))\n",
    "                    .map('rows', ('id', 'score', 'text'), lambda x: (x[0], x[1], x[2])) \n",
    "                    .output('id','score','text')\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6c927d6-d399-4aeb-b5b3-025e03e1f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_voice_extract_embeddings(path1):\n",
    "    wav1, _ = apply_effects_file(path1, EFFECTS)\n",
    "    print(wav1.shape,)\n",
    "\n",
    "    input1 = feature_extractor(wav1.squeeze(0), return_tensors=\"pt\", sampling_rate=16000).input_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb1 = model(input1).embeddings\n",
    "        print(\"mb1 shape\")\n",
    "        print(emb1.shape)\n",
    "    emb1 = torch.nn.functional.normalize(emb1, dim=-1).cpu()\n",
    "    print(\"After normalize emb1 shape\")\n",
    "    print(emb1.shape)\n",
    "    return emb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e444bf08-c969-4ae6-963c-9334b290d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_user(audio):\n",
    "    currentuserembed=user_voice_extract_embeddings(audio)\n",
    "    print(currentuserembed.shape)\n",
    "\n",
    "    for i in range(len(userresult)):\n",
    "        print(userresult[i]['name']+\" Comparing is it this person\")\n",
    "        emb=numpy.array([userresult[i]['embedding']])\n",
    "        temb = torch.from_numpy(emb)\n",
    "        similarity = cosine_sim(currentuserembed,temb).numpy()[0]\n",
    "        if similarity >= 0.85:\n",
    "            return userresult[i]['name']\n",
    "    return \"Anonymouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "62d19a61-183e-4677-a7ad-98dd7a499af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(audio):\n",
    "    current_user=match_user(audio)\n",
    "    print(\"User who spoke is current_user =\"+current_user)\n",
    "    #Emotions\n",
    "    out_prob, score, index, emotion = classifier.classify_file(audio)\n",
    "    contextStr=\"\"\n",
    "    answer=\"\"\n",
    "    print(\"read_audio = \"+audio)\n",
    "    print(\"emotion = \"+emotion[0])\n",
    "    # Milvus search on the text embedding\n",
    "    qtn, embed=get_embedding(audio)\n",
    "    print(\"question = \"+qtn)\n",
    "    res = search_pipe.batch([embed])\n",
    "    ans = DataCollection(res[0])\n",
    "    for i in range(len(ans)):\n",
    "        id=ans[i]['id']\n",
    "        score=ans[i]['score']\n",
    "        text=ans[i]['text']\n",
    "        if (i==0):\n",
    "            answer=text.lower()\n",
    "        print(str(id)+\", \"+str(score))\n",
    "        contextStr=contextStr+text.lower()+\". \"    \n",
    "    print(\"answer=\"+answer)\n",
    "    print(\"context=\"+contextStr)\n",
    "    # Running the TTS\n",
    "    mel_output, mel_length, alignment = tacotron2.encode_text(answer)\n",
    "\n",
    "    # Running Vocoder (spectrogram-to-waveform)\n",
    "    waveforms = hifi_gan.decode_batch(mel_output)\n",
    "\n",
    "    # Save the waverform\n",
    "    torchaudio.save(audio,waveforms.squeeze(1), 22050)\n",
    "\n",
    "    \n",
    "    query_response={\"current_user\": current_user, \"id\": id, \"score\": score, \"question\": qtn, \"emotion\":emotion[0], \n",
    "                    \"answer\": answer, \"context\": contextStr, \"response_audio\": audio }  \n",
    "\n",
    "    return current_user,emotion[0],qtn,audio,score,answer,contextStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aac50238-41a2-44cd-b2c0-524eeea9f0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_16028/911653866.py:2: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  gradio.inputs.Audio(source=\"microphone\", type=\"filepath\", optional=True, label=\"Ask Question\")\n",
      "/var/tmp/ipykernel_16028/911653866.py:2: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  gradio.inputs.Audio(source=\"microphone\", type=\"filepath\", optional=True, label=\"Ask Question\")\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    gradio.inputs.Audio(source=\"microphone\", type=\"filepath\", optional=True, label=\"Ask Question\")\n",
    "]\n",
    "output = [\n",
    "    gradio.Textbox(label=\"Current User\"),\n",
    "    gradio.Textbox(label=\"Emotion\"),\n",
    "    gradio.Textbox(label=\"Question\"),\n",
    "    gradio.Audio(label=\"Response Audio\", type=\"filepath\"),\n",
    "    gradio.Textbox(label=\"Score\"),\n",
    "    gradio.Textbox(label=\"Answer Text\"),\n",
    "    gradio.Textbox(label=\"Context\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6c84c2d-d9a1-4d45-a050-d03e20ac5f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_16028/843003908.py:1: GradioDeprecationWarning: `layout` parameter is deprecated, and it has no effect\n",
      "  interface = gradio.Interface(\n",
      "/opt/conda/lib/python3.10/site-packages/gradio/interface.py:328: UserWarning: The `allow_flagging` parameter in `Interface` nowtakes a string value ('auto', 'manual', or 'never'), not a boolean. Setting parameter to: 'never'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7875\n",
      "Running on public URL: https://c3b2f17214bec32735.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c3b2f17214bec32735.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/gradio/processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 42061])\n",
      "mb1 shape\n",
      "torch.Size([1, 512])\n",
      "After normalize emb1 shape\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "user/Samantha Comparing is it this person\n",
      "user/Sridhanya Comparing is it this person\n",
      "User who spoke is current_user =user/Sridhanya\n",
      "read_audio = /var/tmp/gradio/5d8610151d2962b50ac86593e2159ac802e44ca0/audio-0-100.wav\n",
      "emotion = neu\n",
      "question = WHAT IS DELIVERY POINT VALIDATION\n",
      "441360469201835733, 0.45261478424072266\n",
      "441360469201835750, 0.6864280104637146\n",
      "441360469201835835, 1.4276584386825562\n",
      "answer=delivery point validation that is dpv is a usps technology that validates address information down to the individual mailing address without dpv verification is individual address is within a range of valid addresses\n",
      "context=delivery point validation that is dpv is a usps technology that validates address information down to the individual mailing address without dpv verification is individual address is within a range of valid addresses. delivery point validation that is dpv and by using dpv one can reduce undeliverable as address that is uamail thereby reducing postage costs and other business costs associated with inaccurate address information. close mode imposes additional rules to insure compliance with the uspscas regulations use this mode to standardize your input for mailing this mode does not perform intersection building name or spatial areas or matches. \n"
     ]
    }
   ],
   "source": [
    "interface = gradio.Interface(\n",
    "    fn=read_audio,\n",
    "    inputs=inputs,\n",
    "    outputs=output,\n",
    "    layout=\"horizontal\",\n",
    "    allow_flagging=False,\n",
    "    live=False,\n",
    "    cache_examples=False\n",
    ")\n",
    "interface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa154e75-32a2-4e78-bb27-8749dd9d28e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(\"/var/tmp/gradio/56b9b711429eb5668e18728741123aab1a3d915c/audio-0-100.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb8d88-d6e8-4892-9d20-c2d1b9f7b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    print(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef644fd-623d-4346-aaff-5719f2243bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
